{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cartmarsh/TwitterSentiment/blob/main/05_cifar10_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glYHP4-7Jp--"
      },
      "source": [
        "# Image Classification using Convolutional Neural Networks in PyTorch\n",
        "\n",
        "### Part 5 of \"Deep Learning with Pytorch: Zero to GANs\"\n",
        "\n",
        "This tutorial series is a hands-on beginner-friendly introduction to deep learning using [PyTorch](https://pytorch.org), an open-source neural networks library. These tutorials take a practical and coding-focused approach. The best way to learn the material is to execute the code and experiment with it yourself. Check out the full series here:\n",
        "\n",
        "1. [PyTorch Basics: Tensors & Gradients](https://jovian.ai/aakashns/01-pytorch-basics)\n",
        "2. [Gradient Descent & Linear Regression](https://jovian.ai/aakashns/02-linear-regression)\n",
        "3. [Working with Images & Logistic Regression](https://jovian.ai/aakashns/03-logistic-regression) \n",
        "4. [Training Deep Neural Networks on a GPU](https://jovian.ai/aakashns/04-feedforward-nn)\n",
        "5. [Image Classification using Convolutional Neural Networks](https://jovian.ai/aakashns/05-cifar10-cnn)\n",
        "6. [Data Augmentation, Regularization and ResNets](https://jovian.ai/aakashns/05b-cifar10-resnet)\n",
        "7. [Generating Images using Generative Adversarial Networks](https://jovian.ai/aakashns/06b-anime-dcgan/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqUkI41WKEM1"
      },
      "source": [
        "This tutorial covers the following topics: \n",
        "\n",
        "- Downloading an image dataset from web URL\n",
        "- Understanding convolution and pooling layers\n",
        "- Creating a convolutional neural network (CNN) using PyTorch\n",
        "- Training a CNN from scratch and monitoring performance\n",
        "- Underfitting, overfitting and how to overcome them"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FTYKRrdK9qJ"
      },
      "source": [
        "### How to run the code\n",
        "\n",
        "This tutorial is an executable [Jupyter notebook](https://jupyter.org) hosted on [Jovian](https://www.jovian.ai). You can _run_ this tutorial and experiment with the code examples in a couple of ways: *using free online resources* (recommended) or *on your computer*.\n",
        "\n",
        "#### Option 1: Running using free online resources (1-click, recommended)\n",
        "\n",
        "The easiest way to start executing the code is to click the **Run** button at the top of this page and select **Run on Colab**. [Google Colab](https://colab.research.google.com) is a free online platform for running Jupyter notebooks using Google's cloud infrastructure. You can also select \"Run on Binder\" or \"Run on Kaggle\" if you face issues running the notebook on Google Colab. \n",
        "\n",
        "\n",
        "#### Option 2: Running on your computer locally\n",
        "\n",
        "To run the code on your computer locally, you'll need to set up [Python](https://www.python.org), download the notebook and install the required libraries. We recommend using the [Conda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/) distribution of Python. Click the **Run** button at the top of this page, select the **Run Locally** option, and follow the instructions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HP67kosYLAlQ"
      },
      "source": [
        "### Using a GPU for faster training\n",
        "\n",
        "You can use a [Graphics Processing Unit](https://en.wikipedia.org/wiki/Graphics_processing_unit) (GPU) to train your models faster if your execution platform is connected to a GPU manufactured by NVIDIA. Follow these instructions to use a GPU on the platform of your choice:\n",
        "\n",
        "* _Google Colab_: Use the menu option \"Runtime > Change Runtime Type\" and select \"GPU\" from the \"Hardware Accelerator\" dropdown.\n",
        "* _Kaggle_: In the \"Settings\" section of the sidebar, select \"GPU\" from the \"Accelerator\" dropdown. Use the button on the top-right to open the sidebar.\n",
        "* _Binder_: Notebooks running on Binder cannot use a GPU, as the machines powering Binder aren't connected to any GPUs.\n",
        "* _Linux_: If your laptop/desktop has an NVIDIA GPU (graphics card), make sure you have installed the [NVIDIA CUDA drivers](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html).\n",
        "* _Windows_: If your laptop/desktop has an NVIDIA GPU (graphics card), make sure you have installed the [NVIDIA CUDA drivers](https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html).\n",
        "* _macOS_: macOS is not compatible with NVIDIA GPUs\n",
        "\n",
        "\n",
        "If you do not have access to a GPU or aren't sure what it is, don't worry, you can execute all the code in this tutorial just fine without a GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQxJbIfeJp-_"
      },
      "source": [
        "## Exploring the CIFAR10 Dataset\n",
        "\n",
        "In the [previous tutorial](https://jovian.ml/aakashns/04-feedforward-nn), we trained a feedfoward neural networks with a single hidden layer to classify handwritten digits from the [MNIST dataset](http://yann.lecun.com/exdb/mnist) with over 97% accuracy. For this tutorial, we'll use the CIFAR10 dataset, which consists of 60000 32x32 px colour images in 10 classes. Here are some sample images from the dataset:\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/709/1*LyV7_xga4jUHdx4_jHk1PQ.png\" style=\"max-width:480px\">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sFBq-ur9Jp-_"
      },
      "outputs": [],
      "source": [
        "# Uncomment and run the appropriate command for your operating system, if required\n",
        "\n",
        "# Linux / Binder / Windows (No GPU)\n",
        "# !pip install numpy matplotlib torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# Linux / Windows (GPU)\n",
        "# pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n",
        " \n",
        "# MacOS (NO GPU)\n",
        "# !pip install numpy matplotlib torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TkOhVp1FJp_A"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import tarfile\n",
        "from torchvision.datasets.utils import download_url\n",
        "from torch.utils.data import random_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ImubC3HRJp_A"
      },
      "outputs": [],
      "source": [
        "project_name='05-cifar10-cnn'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkBn_XvbJp_A"
      },
      "source": [
        "We'll download the images in PNG format from [this page](https://course.fast.ai/datasets), using some helper functions from the `torchvision` and `tarfile` packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "39ad149962ac4049bd8e912374256fab",
            "b8c79e9aaa694b40ac566d647035d95c",
            "b80f56be8cd34a33bdb1223579a44db6",
            "7c61d90edbba481aaaa7ad19d400858b",
            "a0daf19c8ae6459d9c3d27f65eca41fa",
            "553ba1e7efd74662aac359f90eb3ea19",
            "a1986035e82243808fae5379bd181638",
            "1a8f177314c14f23bd7773647e84d930",
            "178ffad1811941108e1e7e40b85ac835",
            "650911e1f3d347f2a36a8ecbf7f945e8",
            "dfb3f252849e40e999f209abcf2560c8"
          ]
        },
        "id": "xazKUTC4Jp_A",
        "outputId": "51e13868-5e8a-422a-f1d8-e9d7dad67d50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://s3.amazonaws.com/fast-ai-imageclas/cifar10.tgz to ./cifar10.tgz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/135107811 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "39ad149962ac4049bd8e912374256fab"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Dowload the dataset\n",
        "dataset_url = \"https://s3.amazonaws.com/fast-ai-imageclas/cifar10.tgz\"\n",
        "download_url(dataset_url, '.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LF_8NbHiJp_C"
      },
      "outputs": [],
      "source": [
        "# Extract from archive\n",
        "with tarfile.open('./cifar10.tgz', 'r:gz') as tar:\n",
        "    tar.extractall(path='./data')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJ86QfBlJp_C"
      },
      "source": [
        "The dataset is extracted to the directory `data/cifar10`. It contains 2 folders `train` and `test`, containing the training set (50000 images) and test set (10000 images) respectively. Each of them contains 10 folders, one for each class of images. Let's verify this using `os.listdir`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ewt8o-hiJp_C",
        "outputId": "eeb84dc0-3748-4b4d-e7d2-44607de14710"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['train', 'test']\n",
            "['ship', 'automobile', 'deer', 'bird', 'horse', 'airplane', 'truck', 'cat', 'dog', 'frog']\n"
          ]
        }
      ],
      "source": [
        "data_dir = './data/cifar10'\n",
        "\n",
        "print(os.listdir(data_dir))\n",
        "classes = os.listdir(data_dir + \"/train\")\n",
        "print(classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tzHkgTDJp_C"
      },
      "source": [
        "Let's look inside a couple of folders, one from the training set and another from the test set. As an exercise, you can verify that that there are an equal number of images for each class, 5000 in the training set and 1000 in the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZS3sDJ4Jp_D",
        "outputId": "7ff72c96-ddd0-4e22-e43a-69924821274f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of training examples for airplanes: 5000\n",
            "['3167.png', '0962.png', '2112.png', '1692.png', '2882.png']\n"
          ]
        }
      ],
      "source": [
        "airplane_files = os.listdir(data_dir + \"/train/airplane\")\n",
        "print('No. of training examples for airplanes:', len(airplane_files))\n",
        "print(airplane_files[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvhUgwAMJp_D",
        "outputId": "853f35bf-12ad-4481-cd67-3ebf5be9e61d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of test examples for ship: 1000\n",
            "['0962.png', '0497.png', '0465.png', '0644.png', '0235.png']\n"
          ]
        }
      ],
      "source": [
        "ship_test_files = os.listdir(data_dir + \"/test/ship\")\n",
        "print(\"No. of test examples for ship:\", len(ship_test_files))\n",
        "print(ship_test_files[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wF5kv2I3Jp_D"
      },
      "source": [
        "The above directory structure (one folder per class) is used by many computer vision datasets, and most deep learning libraries provide utilites for working with such datasets. We can use the `ImageFolder` class from `torchvision` to load the data as PyTorch tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ubxtJzJjJp_E"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import ToTensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6IAxD0e9Jp_E"
      },
      "outputs": [],
      "source": [
        "dataset = ImageFolder(data_dir+'/train', transform=ToTensor())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilbPijY3Jp_F"
      },
      "source": [
        "Let's look at a sample element from the training dataset. Each element is a tuple, containing a image tensor and a label. Since the data consists of 32x32 px color images with 3 channels (RGB), each image tensor has the shape `(3, 32, 32)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFWkd84bJp_F",
        "outputId": "6b063219-1042-4ee9-efcf-b907c48a2d1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 32, 32]) 0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.7922, 0.7922, 0.8000,  ..., 0.8118, 0.8039, 0.7961],\n",
              "         [0.8078, 0.8078, 0.8118,  ..., 0.8235, 0.8157, 0.8078],\n",
              "         [0.8235, 0.8275, 0.8314,  ..., 0.8392, 0.8314, 0.8235],\n",
              "         ...,\n",
              "         [0.8549, 0.8235, 0.7608,  ..., 0.9529, 0.9569, 0.9529],\n",
              "         [0.8588, 0.8510, 0.8471,  ..., 0.9451, 0.9451, 0.9451],\n",
              "         [0.8510, 0.8471, 0.8510,  ..., 0.9373, 0.9373, 0.9412]],\n",
              "\n",
              "        [[0.8000, 0.8000, 0.8078,  ..., 0.8157, 0.8078, 0.8000],\n",
              "         [0.8157, 0.8157, 0.8196,  ..., 0.8275, 0.8196, 0.8118],\n",
              "         [0.8314, 0.8353, 0.8392,  ..., 0.8392, 0.8353, 0.8275],\n",
              "         ...,\n",
              "         [0.8510, 0.8196, 0.7608,  ..., 0.9490, 0.9490, 0.9529],\n",
              "         [0.8549, 0.8471, 0.8471,  ..., 0.9412, 0.9412, 0.9412],\n",
              "         [0.8471, 0.8431, 0.8471,  ..., 0.9333, 0.9333, 0.9333]],\n",
              "\n",
              "        [[0.7804, 0.7804, 0.7882,  ..., 0.7843, 0.7804, 0.7765],\n",
              "         [0.7961, 0.7961, 0.8000,  ..., 0.8039, 0.7961, 0.7882],\n",
              "         [0.8118, 0.8157, 0.8235,  ..., 0.8235, 0.8157, 0.8078],\n",
              "         ...,\n",
              "         [0.8706, 0.8392, 0.7765,  ..., 0.9686, 0.9686, 0.9686],\n",
              "         [0.8745, 0.8667, 0.8627,  ..., 0.9608, 0.9608, 0.9608],\n",
              "         [0.8667, 0.8627, 0.8667,  ..., 0.9529, 0.9529, 0.9529]]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "img, label = dataset[0]\n",
        "print(img.shape, label)\n",
        "img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KylRsYg2Jp_F"
      },
      "source": [
        "The list of classes is stored in the `.classes` property of the dataset. The numeric label for each element corresponds to index of the element's label in the list of classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-XOdz6KJp_G",
        "outputId": "d1ae6aef-7d63-4861-a26a-1db13db38bd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
          ]
        }
      ],
      "source": [
        "print(dataset.classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwAIns3lJp_G"
      },
      "source": [
        "We can view the image using `matplotlib`, but we need to change the tensor dimensions to `(32,32,3)`. Let's create a helper function to display an image and its label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "JC8sRIySO187"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "matplotlib.rcParams['figure.facecolor'] = '#ffffff'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "6TM5cZVyJp_G"
      },
      "outputs": [],
      "source": [
        "\n",
        "def show_example(img, label):\n",
        "    print('Label: ', dataset.classes[label], \"(\"+str(label)+\")\")\n",
        "    plt.imshow(img.permute(1, 2, 0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_Fhylr7Jp_H"
      },
      "source": [
        "Let's look at a couple of images from the dataset. As you can tell, the 32x32px images are quite difficult to identify, even for the human eye. Try changing the indices below to view different images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "DLd1lDT0Jp_H",
        "outputId": "aa10a63b-de15-40da-fb51-455949dd375a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label:  airplane (0)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "show_example(*dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "C-bLFBZ0Jp_H",
        "outputId": "30716396-2114-440b-9107-926dd7bc364b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label:  airplane (0)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "show_example(*dataset[1099])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMpExaTwJp_I"
      },
      "source": [
        "### Save and upload your notebook\n",
        "\n",
        "Whether you're running this Jupyter notebook online or on your computer, it's essential to save your work from time to time. You can continue working on a saved notebook later or share it with friends and colleagues to let them execute your code. [Jovian](https://jovian.ai/platform-features) offers an easy way of saving and sharing your Jupyter notebooks online."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "YZQLAkj-Jp_I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bc2830c-3533-48c0-e3e9-6ebd023f515a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |████▊                           | 10 kB 27.3 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 20 kB 15.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 30 kB 21.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 40 kB 15.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 51 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 61 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 68 kB 4.6 MB/s \n",
            "\u001b[?25h  Building wheel for uuid (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install jovian --upgrade -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Lz9-PiH6Jp_I"
      },
      "outputs": [],
      "source": [
        "import jovian"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UdJfJBIJp_I",
        "outputId": "c858eea0-1686-4cfd-f72f-b9d5ffe2ac1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[jovian] Detected Colab notebook...\u001b[0m\n",
            "[jovian] jovian.commit() is no longer required on Google Colab. If you ran this notebook from Jovian, \n",
            "then just save this file in Colab using Ctrl+S/Cmd+S and it will be updated on Jovian. \n",
            "Also, you can also delete this cell, it's no longer necessary.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "jovian.commit(project=project_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvBIchACJp_I"
      },
      "source": [
        "`jovian.commit` uploads the notebook to your Jovian account, captures the Python environment, and creates a shareable link for your notebook, as shown above. You can use this link to share your work and let anyone (including you) run your notebooks and reproduce your work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxuW5ucGJp_J"
      },
      "source": [
        "## Training and Validation Datasets\n",
        "\n",
        "While building real world machine learning models, it is quite common to split the dataset into 3 parts:\n",
        "\n",
        "1. **Training set** - used to train the model i.e. compute the loss and adjust the weights of the model using gradient descent.\n",
        "2. **Validation set** - used to evaluate the model while training, adjust hyperparameters (learning rate etc.) and pick the best version of the model.\n",
        "3. **Test set** - used to compare different models, or different types of modeling approaches, and report the final accuracy of the model.\n",
        "\n",
        "Since there's no predefined validation set, we can set aside a small portion (5000 images) of the training set to be used as the validation set. We'll use the `random_split` helper method from PyTorch to do this. To ensure that we always create the same validation set, we'll also set a seed for the random number generator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "MSVGdRK3Jp_J"
      },
      "outputs": [],
      "source": [
        "random_seed = 42\n",
        "torch.manual_seed(random_seed);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yy5ekvZOJp_L",
        "outputId": "860ee1b9-db07-4600-85c1-279478ac88fd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(45000, 5000)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "val_size = 5000\n",
        "train_size = len(dataset) - val_size\n",
        "\n",
        "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
        "len(train_ds), len(val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aJ9STnUJp_L"
      },
      "source": [
        "The `jovian` library also provides a simple API for recording important parameters related to the dataset, model training, results etc. for easy reference and comparison between multiple experiments. Let's record `dataset_url`, `val_pct` and `rand_seed` using `jovian.log_dataset`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83Z33gU7Jp_M",
        "outputId": "1748d267-7edc-48ce-c1ea-f8f87698de57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[jovian] Please enter your API key ( from https://jovian.ai/ ):\u001b[0m\n",
            "API KEY: "
          ]
        }
      ],
      "source": [
        "jovian.log_dataset(dataset_url=dataset_url, val_size=val_size, random_seed=random_seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8Hu-d6mJp_M"
      },
      "source": [
        "We can now create data loaders for training and validation, to load the data in batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wM8WQFfJp_M"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data.dataloader import DataLoader\n",
        "\n",
        "batch_size=128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWXX_hHGJp_M"
      },
      "outputs": [],
      "source": [
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_dl = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YM8SZq7Jp_M"
      },
      "source": [
        "We can look at batches of images from the dataset using the `make_grid` method from `torchvision`. Each time the following code is run, we get a different bach, since the sampler shuffles the indices before creating batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEYh1NZxJp_N"
      },
      "outputs": [],
      "source": [
        "from torchvision.utils import make_grid\n",
        "\n",
        "def show_batch(dl):\n",
        "    for images, labels in dl:\n",
        "        fig, ax = plt.subplots(figsize=(12, 6))\n",
        "        ax.set_xticks([]); ax.set_yticks([])\n",
        "        ax.imshow(make_grid(images, nrow=16).permute(1, 2, 0))\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMSe7bd8Jp_N"
      },
      "outputs": [],
      "source": [
        "show_batch(train_dl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAzaYnunJp_N"
      },
      "source": [
        "Once again, let's save and commit our work using `jovian` before proceeding further."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5O2QNSGJp_O"
      },
      "outputs": [],
      "source": [
        "jovian.commit(project=project_name, environment=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isNOQSEgJp_O"
      },
      "source": [
        "After the first commit, all subsequent commits record a new version of the notebook within the same Jovian project. You can use `jovian.commit` to version Jupyter notebooks (instead of doing `File > Save As`), and keep your data science projects organized. Also check out the [**Records**](https://jovian.ml/aakashns/05-cifar10-cnn/v/2/records) tab on the project page to see how the information logged using `jovian.log_dataset` appears on the UI.\n",
        "\n",
        "<a href=\"https://jovian.ml/aakashns/05-cifar10-cnn/v/2/records\"><img src=\"https://i.imgur.com/h0zkmn9.png\" style=\"width:400px\" ></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkRCd9f4Jp_O"
      },
      "source": [
        "## Defining the Model (Convolutional Neural Network)\n",
        "\n",
        "In our [previous tutorial](https://jovian.ml/aakashns/04-feedforward-nn), we defined a deep neural network with fully-connected layers using `nn.Linear`. For this tutorial however, we will use a convolutional neural network, using the `nn.Conv2d` class from PyTorch.\n",
        "\n",
        "> The 2D convolution is a fairly simple operation at heart: you start with a kernel, which is simply a small matrix of weights. This kernel “slides” over the 2D input data, performing an elementwise multiplication with the part of the input it is currently on, and then summing up the results into a single output pixel. - [Source](https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1)\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1070/1*Zx-ZMLKab7VOCQTxdZ1OAw.gif\" style=\"max-width:400px;\">\n",
        "\n",
        "\n",
        "Let us implement a convolution operation on a 1 channel image with a 3x3 kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tc-SGGw7Jp_O"
      },
      "outputs": [],
      "source": [
        "def apply_kernel(image, kernel):\n",
        "    ri, ci = image.shape       # image dimensions\n",
        "    rk, ck = kernel.shape      # kernel dimensions\n",
        "    ro, co = ri-rk+1, ci-ck+1  # output dimensions\n",
        "    output = torch.zeros([ro, co])\n",
        "    for i in range(ro): \n",
        "        for j in range(co):\n",
        "            output[i,j] = torch.sum(image[i:i+rk,j:j+ck] * kernel)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Kp_anXMJp_P"
      },
      "outputs": [],
      "source": [
        "sample_image = torch.tensor([\n",
        "    [3, 3, 2, 1, 0], \n",
        "    [0, 0, 1, 3, 1], \n",
        "    [3, 1, 2, 2, 3], \n",
        "    [2, 0, 0, 2, 2], \n",
        "    [2, 0, 0, 0, 1]\n",
        "], dtype=torch.float32)\n",
        "\n",
        "sample_kernel = torch.tensor([\n",
        "    [0, 1, 2], \n",
        "    [2, 2, 0], \n",
        "    [0, 1, 2]\n",
        "], dtype=torch.float32)\n",
        "\n",
        "apply_kernel(sample_image, sample_kernel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZFVHVT7Jp_P"
      },
      "source": [
        "For multi-channel images, a different kernel is applied to each channels, and the outputs are added together pixel-wise. \n",
        "\n",
        "Checking out the following articles to gain a better understanding of convolutions:\n",
        "\n",
        "1. [Intuitively understanding Convolutions for Deep Learning](https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1) by Irhum Shafkat\n",
        "2. [Convolutions in Depth](https://sgugger.github.io/convolution-in-depth.html) by Sylvian Gugger (this article implements convolutions from scratch)\n",
        "\n",
        "There are certain advantages offered by convolutional layers when working with image data:\n",
        "\n",
        "* **Fewer parameters**: A small set of parameters (the kernel) is used to calculate outputs of the entire image, so the model has much fewer parameters compared to a fully connected layer. \n",
        "* **Sparsity of connections**: In each layer, each output element only depends on a small number of input elements, which makes the forward and backward passes more efficient.\n",
        "* **Parameter sharing and spatial invariance**: The features learned by a kernel in one part of the image can be used to detect similar pattern in a different part of another image.\n",
        "\n",
        "We will also use a [max-pooling](https://computersciencewiki.org/index.php/Max-pooling_/_Pooling) layers to progressively decrease the height & width of the output tensors from each convolutional layer.\n",
        "\n",
        "<img src=\"https://computersciencewiki.org/images/8/8a/MaxpoolSample2.png\" style=\"max-width:400px;\">\n",
        "\n",
        "Before we define the entire model, let's look at how a single convolutional layer followed by a max-pooling layer operates on the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKaWyJtUJp_P"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXmQxqS0Jp_P"
      },
      "outputs": [],
      "source": [
        "simple_model = nn.Sequential(\n",
        "    nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1),\n",
        "    nn.MaxPool2d(2, 2)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnuN34GwJp_P"
      },
      "source": [
        "Refer to [Sylvian's post](https://sgugger.github.io/convolution-in-depth.html) for an explanation of `kernel_size`, `stride` and `padding`.    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyxDOGH_Jp_P"
      },
      "outputs": [],
      "source": [
        "for images, labels in train_dl:\n",
        "    print('images.shape:', images.shape)\n",
        "    out = simple_model(images)\n",
        "    print('out.shape:', out.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSKyZ2xIJp_P"
      },
      "source": [
        "The `Conv2d` layer transforms a 3-channel image to a 16-channel *feature map*, and the `MaxPool2d` layer halves the height and width. The feature map gets smaller as we add more layers, until we are finally left with a small feature map, which can be flattened into a vector. We can then add some fully connected layers at the end to get vector of size 10 for each image.\n",
        "\n",
        "<img src=\"https://i.imgur.com/KKtPOKE.png\" style=\"max-width:540px\">\n",
        "\n",
        "Let's define the model by extending an `ImageClassificationBase` class which contains helper methods for training & validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgbZ6gLOJp_Q"
      },
      "outputs": [],
      "source": [
        "class ImageClassificationBase(nn.Module):\n",
        "    def training_step(self, batch):\n",
        "        images, labels = batch \n",
        "        out = self(images)                  # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch):\n",
        "        images, labels = batch \n",
        "        out = self(images)                    # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
        "        acc = accuracy(out, labels)           # Calculate accuracy\n",
        "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
        "        \n",
        "    def validation_epoch_end(self, outputs):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
        "        batch_accs = [x['val_acc'] for x in outputs]\n",
        "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
        "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
        "    \n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
        "            epoch, result['train_loss'], result['val_loss'], result['val_acc']))\n",
        "        \n",
        "def accuracy(outputs, labels):\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rw-AyZcjJp_Q"
      },
      "source": [
        "\n",
        "We'll use `nn.Sequential` to chain the layers and activations functions into a single network architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4xrvghlJp_Q"
      },
      "outputs": [],
      "source": [
        "class Cifar10CnnModel(ImageClassificationBase):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n",
        "\n",
        "            nn.Flatten(), \n",
        "            nn.Linear(256*4*4, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10))\n",
        "        \n",
        "    def forward(self, xb):\n",
        "        return self.network(xb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tU5q48KeJp_Q"
      },
      "outputs": [],
      "source": [
        "model = Cifar10CnnModel()\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loJN3i6_Jp_Q"
      },
      "source": [
        "Let's verify that the model produces the expected output on a batch of training data. The 10 outputs for each image can be interpreted as probabilities for the 10 target classes (after applying softmax), and the class with the highest probability is chosen as the label predicted by the model for the input image. Check out [Part 3 (logistic regression)](https://jovian.ml/aakashns/03-logistic-regression#C50) for a more detailed discussion on interpeting the outputs, applying softmax and identifying the predicted labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCm2I82pJp_Q"
      },
      "outputs": [],
      "source": [
        "for images, labels in train_dl:\n",
        "    print('images.shape:', images.shape)\n",
        "    out = model(images)\n",
        "    print('out.shape:', out.shape)\n",
        "    print('out[0]:', out[0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oASDWWc9Jp_R"
      },
      "source": [
        "To seamlessly use a GPU, if one is available, we define a couple of helper functions (`get_default_device` & `to_device`) and a helper class `DeviceDataLoader` to move our model & data to the GPU as required. These are described in more detail in the [previous tutorial](https://jovian.ml/aakashns/04-feedforward-nn#C21)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wR9oVOopJp_R"
      },
      "outputs": [],
      "source": [
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "    \n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "        \n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl: \n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zBM9zOqJp_R"
      },
      "source": [
        "Based on where you're running this notebook, your default device could be a CPU (`torch.device('cpu')`) or a GPU (`torch.device('cuda')`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iczDAm1SJp_R"
      },
      "outputs": [],
      "source": [
        "device = get_default_device()\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdlOEla3Jp_R"
      },
      "source": [
        "We can now wrap our training and validation data loaders using `DeviceDataLoader` for automatically transferring batches of data to the GPU (if available), and use `to_device` to move our model to the GPU (if available)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vx3BnG8IJp_R"
      },
      "outputs": [],
      "source": [
        "train_dl = DeviceDataLoader(train_dl, device)\n",
        "val_dl = DeviceDataLoader(val_dl, device)\n",
        "to_device(model, device);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CICXCTtpJp_R"
      },
      "source": [
        "Once again, let's save and commit the notebook before we proceed further."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "js6r9zuBJp_R"
      },
      "outputs": [],
      "source": [
        "jovian.commit(project=project_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qq2eMlEJp_S"
      },
      "source": [
        "## Training the Model\n",
        "\n",
        "We'll define two functions: `fit` and `evaluate` to train the model using gradient descent and evaluate its performance on the validation set. For a detailed walkthrough of these functions, check out the [previous tutorial](https://jovian.ai/aakashns/03-logistic-regression)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEQkLCaYJp_S"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, val_loader):\n",
        "    model.eval()\n",
        "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
        "    return model.validation_epoch_end(outputs)\n",
        "\n",
        "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
        "    history = []\n",
        "    optimizer = opt_func(model.parameters(), lr)\n",
        "    for epoch in range(epochs):\n",
        "        # Training Phase \n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        for batch in train_loader:\n",
        "            loss = model.training_step(batch)\n",
        "            train_losses.append(loss)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "        # Validation phase\n",
        "        result = evaluate(model, val_loader)\n",
        "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
        "        model.epoch_end(epoch, result)\n",
        "        history.append(result)\n",
        "    return history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGbR0ClAJp_S"
      },
      "source": [
        "Before we begin training, let's instantiate the model once again and see how it performs on the validation set with the initial set of parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOI6pwRmJp_S"
      },
      "outputs": [],
      "source": [
        "model = to_device(Cifar10CnnModel(), device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWO9y9TrJp_S"
      },
      "outputs": [],
      "source": [
        "evaluate(model, val_dl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEuauDpwJp_S"
      },
      "source": [
        "The initial accuracy is around 10%, which is what one might expect from a randomly intialized model (since it has a 1 in 10 chance of getting a label right by guessing randomly).\n",
        "\n",
        "We'll use the following *hyperparmeters* (learning rate, no. of epochs, batch_size etc.) to train our model. As an exercise, you can try changing these to see if you have achieve a higher accuracy in a shorter time. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdSA8RWDJp_S"
      },
      "outputs": [],
      "source": [
        "num_epochs = 10\n",
        "opt_func = torch.optim.Adam\n",
        "lr = 0.001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6lIc9bUJp_S"
      },
      "source": [
        "It's important to record the hyperparameters of every experiment you do, to replicate it later and compare it against other experiments. We can record them using `jovian.log_hyperparams`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfD4is6FJp_S"
      },
      "outputs": [],
      "source": [
        "jovian.reset()\n",
        "jovian.log_hyperparams({\n",
        "    'num_epochs': num_epochs,\n",
        "    'opt_func': opt_func.__name__,\n",
        "    'batch_size': batch_size,\n",
        "    'lr': lr,\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZK4Y2XrJp_T"
      },
      "outputs": [],
      "source": [
        "history = fit(num_epochs, lr, model, train_dl, val_dl, opt_func)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhybBuPkJp_T"
      },
      "source": [
        "Just as we have recorded the hyperparameters, we can also record the final metrics achieved by the model using `jovian.log_metrics` for reference, analysis and comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5Kxuq9EJp_T"
      },
      "outputs": [],
      "source": [
        "jovian.log_metrics(train_loss=history[-1]['train_loss'], \n",
        "                   val_loss=history[-1]['val_loss'], \n",
        "                   val_acc=history[-1]['val_acc'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55K2kRe6Jp_T"
      },
      "source": [
        "We can also plot the valdation set accuracies to study how the model improves over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UP7E5_GhJp_T"
      },
      "outputs": [],
      "source": [
        "def plot_accuracies(history):\n",
        "    accuracies = [x['val_acc'] for x in history]\n",
        "    plt.plot(accuracies, '-x')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.title('Accuracy vs. No. of epochs');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYgnxqQQJp_T"
      },
      "outputs": [],
      "source": [
        "plot_accuracies(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPmvnBsaJp_T"
      },
      "source": [
        "Our model reaches an accuracy of around 75%, and by looking at the graph, it seems unlikely that the model will achieve an accuracy higher than 80% even after training for a long time. This suggests that we might need to use a more powerful model to capture the relationship between the images and the labels more accurately. This can be done by adding more convolutional layers to our model, or incrasing the no. of channels in each convolutional layer, or by using regularization techniques.\n",
        "\n",
        "We can also plot the training and validation losses to study the trend."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oefp3XHjJp_U"
      },
      "outputs": [],
      "source": [
        "def plot_losses(history):\n",
        "    train_losses = [x.get('train_loss') for x in history]\n",
        "    val_losses = [x['val_loss'] for x in history]\n",
        "    plt.plot(train_losses, '-bx')\n",
        "    plt.plot(val_losses, '-rx')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('loss')\n",
        "    plt.legend(['Training', 'Validation'])\n",
        "    plt.title('Loss vs. No. of epochs');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBpeN5YhJp_U"
      },
      "outputs": [],
      "source": [
        "plot_losses(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0ZkttR-Jp_U"
      },
      "source": [
        "Initialy, both the training and validation losses seem to decrease over time. However, if you train the model for long enough, you will notice that the training loss continues to decrease, while the validation loss stops decreasing, and even starts to increase after a certain point! \n",
        "\n",
        "<img src=\"https://i.stack.imgur.com/1QU0m.png\" style=\"max-width:400px;\">\n",
        "\n",
        "This phenomenon is called **overfitting**, and it is the no. 1 why many machine learning models give rather terrible results on real-world data. It happens because the model, in an attempt to minimize the loss, starts to learn patters are are unique to the training data, sometimes even memorizing specific training examples. Because of this, the model does not generalize well to previously unseen data.\n",
        "\n",
        "\n",
        "Following are some common stragegies for avoiding overfitting:\n",
        "\n",
        "- Gathering and generating more training data, or adding noise to it\n",
        "- Using regularization techniques like batch normalization & dropout\n",
        "- Early stopping of model's training, when validation loss starts to increase\n",
        "\n",
        "We will cover these topics in more detail in the next tutorial in this series, and learn how we can reach an accuracy of **over 90%** by making minor but important changes to our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "my239tuTJp_U"
      },
      "source": [
        "Before continuing, let us save our work to the cloud using `jovian.commit`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bLvOFf-Jp_U"
      },
      "outputs": [],
      "source": [
        "jovian.commit(project=project_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jcncNcgJp_U"
      },
      "source": [
        "When you try different experiments (by chaging the learning rate, batch size, optimizer etc.) and record hyperparameters and metrics with each version of your notebook, you can use the [**Compare**](https://jovian.ml/aakashns/05-cifar10-cnn/compare) view on the project page to analyze which approaches are working well and which ones aren't. You sort/filter by accuracy, loss etc., add notes for each version and even invite collaborators to contribute to your project with their own experiments.\n",
        "\n",
        "<a href=\"https://jovian.ml/aakashns/05-cifar10-cnn\"><img src=\"https://i.imgur.com/p1Z3vgN.png\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqN4ozY6Jp_U"
      },
      "source": [
        "## Testing with individual images\n",
        "\n",
        "While we have been tracking the overall accuracy of a model so far, it's also a good idea to look at model's results on some sample images. Let's test out our model with some images from the predefined test dataset of 10000 images. We begin by creating a test dataset using the `ImageFolder` class.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7stwSbarJp_U"
      },
      "outputs": [],
      "source": [
        "test_dataset = ImageFolder(data_dir+'/test', transform=ToTensor())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckEBASy7Jp_U"
      },
      "source": [
        "Let's define a helper function `predict_image`, which returns the predicted label for a single image tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QidsbfBLJp_U"
      },
      "outputs": [],
      "source": [
        "def predict_image(img, model):\n",
        "    # Convert to a batch of 1\n",
        "    xb = to_device(img.unsqueeze(0), device)\n",
        "    # Get predictions from model\n",
        "    yb = model(xb)\n",
        "    # Pick index with highest probability\n",
        "    _, preds  = torch.max(yb, dim=1)\n",
        "    # Retrieve the class label\n",
        "    return dataset.classes[preds[0].item()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_fkqvIDJp_V"
      },
      "outputs": [],
      "source": [
        "img, label = test_dataset[0]\n",
        "plt.imshow(img.permute(1, 2, 0))\n",
        "print('Label:', dataset.classes[label], ', Predicted:', predict_image(img, model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNlbM9l7Jp_V"
      },
      "outputs": [],
      "source": [
        "img, label = test_dataset[1002]\n",
        "plt.imshow(img.permute(1, 2, 0))\n",
        "print('Label:', dataset.classes[label], ', Predicted:', predict_image(img, model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GT5bNsj2Jp_V"
      },
      "outputs": [],
      "source": [
        "img, label = test_dataset[6153]\n",
        "plt.imshow(img.permute(1, 2, 0))\n",
        "print('Label:', dataset.classes[label], ', Predicted:', predict_image(img, model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqZv8qKPJp_V"
      },
      "source": [
        "Identifying where our model performs poorly can help us improve the model, by collecting more training data, increasing/decreasing the complexity of the model, and changing the hypeparameters.\n",
        "\n",
        "As a final step, let's also look at the overall loss and accuracy of the model on the test set, and record using `jovian`. We expect these values to be similar to those for the validation set. If not, we might need a better validation set that has similar data and distribution as the test set (which often comes from real world data)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1hjzFhUJp_V"
      },
      "outputs": [],
      "source": [
        "test_loader = DeviceDataLoader(DataLoader(test_dataset, batch_size*2), device)\n",
        "result = evaluate(model, test_loader)\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACxJDNG9Jp_V"
      },
      "outputs": [],
      "source": [
        "jovian.log_metrics(test_loss=result['val_loss'], test_acc=result['val_acc'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Smk8znXkJp_W"
      },
      "source": [
        "## Saving and loading the model\n",
        "\n",
        "Since we've trained our model for a long time and achieved a resonable accuracy, it would be a good idea to save the weights of the model to disk, so that we can reuse the model later and avoid retraining from scratch. Here's how you can save the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLLm-pIyJp_W"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'cifar10-cnn.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsxZJi5fJp_W"
      },
      "source": [
        "The `.state_dict` method returns an `OrderedDict` containing all the weights and bias matrices mapped to the right attributes of the model. To load the model weights, we can redefine the model with the same structure, and use the `.load_state_dict` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mb9KOT7GJp_W"
      },
      "outputs": [],
      "source": [
        "model2 = to_device(Cifar10CnnModel(), device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFCWPlalJp_W"
      },
      "outputs": [],
      "source": [
        "model2.load_state_dict(torch.load('cifar10-cnn.pth'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o53KprzHJp_W"
      },
      "source": [
        "Just as a sanity check, let's verify that this model has the same loss and accuracy on the test set as before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAtV9ZOqJp_W"
      },
      "outputs": [],
      "source": [
        "evaluate(model2, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrqd6TsvJp_X"
      },
      "source": [
        "Let's make one final commit using `jovian`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANPiy0tfJp_X"
      },
      "outputs": [],
      "source": [
        "jovian.commit(project=project_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3mWqJcOJp_X"
      },
      "source": [
        "Check out the **Files** tab on the project page to view or download the trained model weights. You can also download all the files together using the *Download Zip* option in the *Clone* dropdown.\n",
        "\n",
        "Data science work is often fragmented across many different platforms (Git for code, Dropbox/S3 for datasets & artifacts, spreadsheets for hyperparameters, metrics etc.) which can make it difficult to share and reproduce experiments. Jovian.ml solves this by capturing everyting related to a data science project on a single platform, while providing a seamless workflow for capturing, sharing and reproducting your work. To learn what you can do with Jovian.ml, check out the docs: [https://docs.jovian.ml](https://docs.jovian.ml)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAESHPLiJp_X"
      },
      "source": [
        "## Summary and Further Reading/Exercises\n",
        "\n",
        "We've covered a lot of ground in this tutorial. Here's quick recap of the topics:\n",
        "* Introduction to the CIFAR10 dataset for image classification\n",
        "* Downloading, extracing and loading an image dataset using `torchvision`\n",
        "* Show random batches of images in a grid using `torchvision.utils.make_grid`\n",
        "* Creating a convolutional neural network using with `nn.Conv2d` and `nn.MaxPool2d` layers\n",
        "* Capturing dataset information, metrics and hyperparameters using the `jovian` library\n",
        "* Training a convolutional neural network and visualizing the losses and errors\n",
        "* Understanding overfitting and the strategies for avoiding it (more on this later)\n",
        "* Generating predictions on single images from the test set\n",
        "* Saving and loading the model weights, and attaching them to the eperiment snaptshot using `jovian`\n",
        "\n",
        "There's a lot of scope to experiment here, and I encourage you to use the interactive nature of Jupyter to play around with the various parameters. Here are a few ideas:\n",
        "* Try chaging the hyperparameters to achieve a higher accuracy within fewer epochs. You use the comparison table on the Jovian.ml project page to compare your experiments.\n",
        "* Try adding more convolutional layers, or increasing the number of channels in each convolutional layer\n",
        "* Try using a feedforward neural network and see what's the maximum accuracy you can achieve\n",
        "* Read about some of the startegies mentioned above for reducing overfitting and achieving better results, and try to implement them by looking into the PyTorch docs.\n",
        "* Modify this notebook to train a model for a different dataset (e.g. CIFAR100 or ImageNet)\n",
        "\n",
        "In the next tutorial, we will continue to improve our model's accuracy using techniques like data augmentation, batch normalization and dropout. We will also learn about residual networks (or ResNets), a small but critical change to the model architecture that will significantly boost the performance of our model. Stay tuned!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ViVX5iIxJp_X"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "39ad149962ac4049bd8e912374256fab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b8c79e9aaa694b40ac566d647035d95c",
              "IPY_MODEL_b80f56be8cd34a33bdb1223579a44db6",
              "IPY_MODEL_7c61d90edbba481aaaa7ad19d400858b"
            ],
            "layout": "IPY_MODEL_a0daf19c8ae6459d9c3d27f65eca41fa"
          }
        },
        "b8c79e9aaa694b40ac566d647035d95c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_553ba1e7efd74662aac359f90eb3ea19",
            "placeholder": "​",
            "style": "IPY_MODEL_a1986035e82243808fae5379bd181638",
            "value": "100%"
          }
        },
        "b80f56be8cd34a33bdb1223579a44db6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a8f177314c14f23bd7773647e84d930",
            "max": 135107811,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_178ffad1811941108e1e7e40b85ac835",
            "value": 135107811
          }
        },
        "7c61d90edbba481aaaa7ad19d400858b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_650911e1f3d347f2a36a8ecbf7f945e8",
            "placeholder": "​",
            "style": "IPY_MODEL_dfb3f252849e40e999f209abcf2560c8",
            "value": " 135107811/135107811 [00:04&lt;00:00, 34553050.17it/s]"
          }
        },
        "a0daf19c8ae6459d9c3d27f65eca41fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "553ba1e7efd74662aac359f90eb3ea19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1986035e82243808fae5379bd181638": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a8f177314c14f23bd7773647e84d930": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "178ffad1811941108e1e7e40b85ac835": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "650911e1f3d347f2a36a8ecbf7f945e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfb3f252849e40e999f209abcf2560c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}